import datetime as dt
import io
import os
import random

from abc import ABC, abstractmethod
from typing import Dict, Optional, List

import libcloud
import pandas as pd
import sqlalchemy as sa

from can_tools.scrapers.db_util import TempTable


class CMU:
    def __init__(
        self,
        category="cases",
        measurement="cumulative",
        unit="people",
        age="all",
        race="all",
        sex="all",
    ):
        self.category = category
        self.measurement = measurement
        self.unit = unit
        self.age = age
        self.race = race
        self.sex = sex


class DatasetBase(ABC):
    """
    Attributes
    ----------
    autodag: bool = True
        Whether an airflow dag should be automatically generated for this class

    data_type: str = "general"
        The type of data for this scraper. This is often set to "covid"
        by subclasses

    table_name: str
        The name of the database table where this data should be inserted
    """

    autodag: bool = True
    data_type: str = "general"
    table_name: str

    def __init__(self, execution_dt: dt.datetime):
        # Set execution date information
        self.execution_dt = execution_dt

        if "GCS_TOKEN" in os.environ.keys():
            driver = libcloud.storage.providers.get_driver("google_storage")
            storage = driver()
        else:
            driver = libcloud.storage.providers.get_driver("local")
            storage = driver()
        pass


    def _retrieve_dt(self, tz: str = "US/Eastern") -> pd.Timestamp:
        """Get the current datetime in a specific timezone"""
        out = pd.Timestamp.utcnow().tz_convert(tz).normalize().tz_localize(None)

        return out

    def _retrieve_vintage(self) -> pd.Timestamp:
        """Get the current UTC timestamp, at hourly resolution. Used as "vintage" in db"""
        return pd.Timestamp.utcnow().floor("h")

    def _clean_or_raw(self, raw: Optional[bool] = True) -> str:
        "Returns whether data is clean or raw"
        return "raw" if raw else "clean"

    def _filepath(self, ft: str) -> str:
        """
        Method for determining the file path/file name -- Everything is
        stored using the following conventions:

        * `{classname}` comes from the class name
        * `{execution dt}` is the execution datetime
        * `{ft}` is the file type

        The data will then be stored in the class storage bucket
        at the path:

        `{classname}/{execution dt}.{ft}``

        Parameters
        ----------
        ft : str
            The filetype that the data should be stored as

        Returns
        -------
        fp : str
            The autogenerated name for where the data will
            be stored
        """
        # Set file path pieces
        cn = self.__class__.__name__
        ed = self.execution_dt.strftime("%Y-%m-%d_%H")

        # Set filepath using its components
        fp = f"{cn}/{ed}.ft"

        return fp

    def _get_storage_container(self, ft: str, raw: bool):
        """
        Using the storage bucket insantiated in the `__init__` method
        to find (or create if necessary) the container where the data
        should be stored or read from
        """
        # Get filename and specify whether it's raw or clean data
        fp = self._filepath(ft)
        rc = self._clean_or_raw(raw=True)

        # Connect (or create) to the clean/raw container
        try:
            container = self.storage.get_container(rc)
        except libcloud.storage.types.ContainerDoesNotExistError:
            container = self.storage.create_container(rc)

        return container

    def _read(self, ft: str, raw: Optional[bool] = True):
        """
        The `_read` method reads data from `self.storage` with a
        filepath that depends on whether we want the raw or cleaned
        values

        Parameters
        ----------
        ft : str
            The filetype that the data should be stored as
        raw : str
            `raw` takes the value `"raw"` if we are working with the
            raw data and "clean" if we are working with cleaned data

        Returns
        -------
        data : str
            The data file as a string
        """
        # Try uploading the data (encoded as bytes) to the container
        container = self._get_storage_container(ft, raw)
        file_object = self.storage.upload_object_via_stream(
            datab, container, fp
        )

        data = "".join([x.decode("utf-8") for x in file_object.as_stream()])

        return data

    def _store(self, data: str, ft: str, raw: Optional[bool] = True):
        """
        The `_store` method puts the data into the `self.storage` with
        a filepath that depends on whether we are storing the raw or
        normalized values

        Parameters
        ----------
        data : str
            The data in its raw format as a string
        ft : str
            The filetype that the data should be stored as
        raw : str
            `raw` takes the value `"raw"` if we are working with the
            raw data and "clean" if we are working with cleaned data

        Returns
        -------
        success : bool
            Whether the data was successfully stored in the storage
            bucket
        """
        # Put data into a BytesIO object so we can dump it into cloud
        datab = io.BytesIO(data.encode("utf-8"))

        # Try uploading the data (encoded as bytes) to the container
        container = self._get_storage_container(ft, raw)
        file_object = self.storage.upload_object_via_stream(
            datab, container, fp
        )

        return isinstance(file_object, libcloud.storage.base.Object)

    @abstractmethod
    def _fetch(self):
        """
        The `_fetch` method should retrieve the data in its raw form
        and return the data and the filetype that it should be
        saved as

        Returns
        -------
        data : str
            The data in its raw format as a string
        ft : str
            The filetype that the data should be stored as

        """
        pass

    def fetch(self):
        """
        Fetches the raw data in whatever format it is originally stored
        in and dumps the raw data into storage using the `_store`
        method. If the download does not come in a text file format
        then we store it as a csv

        Returns
        -------
        success: bool
            Takes the value `True` if it successfully downloads and
            stores the data
        """
        data, ft = self._fetch()
        success = self._store(data, ft, raw=True)

        return success

    @abstractmethod
    def _normalize(self, data: str) -> pd.DataFrame:
        """
        The `_normalize` method should take the data in its raw form
        (as a string) and then clean the data

        Parameters
        ----------
        data : str
            The raw data as a str

        Returns
        -------
        df : pd.DataFrame
            The cleaned data as a DataFrame
        """
        pass

    def normalize(self):
        """
        The `_normalize` method should take the data in its raw form
        from the storage bucket, clean the data, and then save the
        cleaned data into the bucket

        Returns
        -------
        success : bool
            A boolean indicating whether the data was successfully
            cleaned
        """
        # Ingest the data
        ft = "?"  # What's the best way to extract ft information
        data = self._read(ft, raw=True)

        # Clean data using `_normalize`
        df = self._normalize(data)

        # TODO: Use tempfile to write to parquet?
        clean_data = df.to_csv(index=False)

        # Push to cleaned
        success = self._store(clean_data, ft, raw=False)

        return True

    @abstractmethod
    def _validate(self):
        pass
    def validate(self):
        pass

    def put(self, connstr):
        """
        Put the DataFrame into the Postgres database

        Parameters
        ----------
        connstr : str
            The sqlalchemy connection URI to be used to connect to the database

        Notes
        -----
        Must be implemented by subclass

        """
        pass

    def extract_CMU(
        self,
        df: pd.DataFrame,
        cmu: Dict[str, CMU],
        columns: List[str] = ["category", "measurement", "unit", "age", "race", "sex"],
        var_name: str = "variable",
    ) -> pd.DataFrame:
        """
        Adds columns "category", "measurement", and "unit" to df

        Parameters
        ----------
        df : pd.DataFrame
            This DataFrame must have the column `variable` and the
            unique elements of `variable` must be keys in the
            `cmu_dict`
        cmu : dict(str -> CMU)
            This dictionary maps variable names into a subcategory,
            measurement, and unit using a CMU namedtuple
        columns: List[str]
            A list of columns to set using the `cmu` dict
        var_name: str
            The name of the column in `df` that should be used to lookup
            items from the `cmu` dict for unpacking columns

        Returns
        -------
        df : pd.DataFrame
            A copy of the DataFrame passed in that has new columns
            "category", "measurement", and "unit"
        """
        foo = df[var_name]
        return df.assign(
            **{c: foo.map(lambda x: cmu[x].__getattribute__(c)) for c in columns}
        )


def build_on_conflict_do_nothing_query(
    columns: List[str],
    dest_table: str,
    temp_table: str,
    unique_constraint: str,
    dest_schema: str = "data",
) -> str:
    """
    Construct sql query to insert data originally from `df` into `dest_schema.dest_table` via the
    temporary database table `temp_table`.

    If there are any conflicts on the unique index `pk`, do nothing

    Parameters
    ----------
    columns: List[str]
        A list of column names found in the temporary table that should be inserted into the final table
    dest_table : str
        The destination table
    temp_table : str
        The temporary table name (in public schema)
    unique_constraint : str
        A string referencing the unique key for the destination table
    dest_schema : str
        The name of the postgres schema for the destination

    Returns
    -------
    query: str
        The SQL query for copying data from the tempmorary table to the destination one

    """
    colnames = ", ".join(columns)
    cols = "(" + colnames + ")"
    if not unique_constraint.startswith("("):
        unique_constraint = f"({unique_constraint})"

    return f"""
    INSERT INTO {dest_schema}.{dest_table} {cols}
    SELECT {colnames} from {temp_table}
    ON CONFLICT {unique_constraint} DO NOTHING;
    """


class InsertWithTempTableMixin:
    """
    Mixin providing ability to `put` a DataFrame into the database
    by first uploading the DataFrame to a temporary table within the database
    then issuing a query to `INSERT INTO ... SELECT...`

    This gives opportunity for the programmer to join in foreign keys from other tables
    when uploading data

    """

    pk: str
    table_name: str

    def _insert_query(
        self, df: pd.DataFrame, table_name: str, temp_name: str, pk: str
    ) -> None:
        """
        Construct query for copying data from temporary table into the official table

        By default it is build_conflict_do_nothing_query, but can be overridden by
        classes using this Mixin

        Parameters
        ----------
        df, table_name, temp_name, pk
            See build_on_conflict_do_nothing_query

        Returns
        -------
        None

        """

        return build_on_conflict_do_nothing_query(df, table_name, temp_name, pk)

    def _put(self, connstr: str, df: pd.DataFrame, table_name: str, pk: str) -> None:
        temp_name = "__" + table_name + str(random.randint(1000, 9999))

        with sa.create_engine(connstr).connect() as conn:
            kw = dict(temp=False, if_exists="replace", destroy=True)

            with TempTable(df, temp_name, conn, **kw):
                sql = self._insert_query(df, table_name, temp_name, pk)
                res = conn.execute(sql)
                print(f"Inserted {res.rowcount} rows into {table_name}")

    def put(self, connstr: str, df: pd.DataFrame) -> None:
        """
        Upload DataFrame `df` to table via a temp table + "insert into ... select" query

        Parameters
        ----------
        connstr : str
            String containing connection URI for connecting to postgres database
        df : pd.DataFrame
            pandas DataFrame containing data to be uploaded

        Returns
        -------

        """
        if df is None:
            if hasattr(self, "df"):
                df = self.df
            else:
                raise ValueError("No df found, please pass")

        if not hasattr(self, "pk"):
            msg = "field `pk` must be set on subclass of OnConflictNothingBase"
            raise ValueError(msg)

        self._put(connstr, df, self.table_name, self.pk)
